{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-J9dvmz-JbK"
   },
   "source": [
    "# Deep Learning Frameworks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0btGdz3-Ptr"
   },
   "source": [
    "Great job on completing the neural network assignment! Well, we know the pain of working with neural networks from scratch. Don't fret! - because there are multiple frameworks, like Tensorflow, Keras, Pytorch, Caffe, etc. out there to help you build a deep learning model easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCxc_AeBAKDn"
   },
   "source": [
    "# Why Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLxkBRcMANHF"
   },
   "source": [
    "Keras is an open source deep learning framework for python. It is a high level API that is built upon Tensorflow. Keras makes deployment of neural networks really simple, with just a few lines of code! The syntax is clear, simple and intuitive.  We urge you to go through the documentation after this to get a better feel of Keras. The link is: https://keras.io/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GRWp2rrp2AK"
   },
   "source": [
    "#Importing Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K25nLJeRIFSL"
   },
   "source": [
    "We will first import tensorflow. Note that Keras uses Tensorflow 2.  as backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_U64Pp7IOae"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sq6aaZ0dEJTL"
   },
   "source": [
    "# The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTLfzHLFENcN"
   },
   "source": [
    "We will generate a random n-class classification problem using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "in3X10QhEYM1",
    "outputId": "0a5e9ab3-8374-41ce-991d-92a6fd3edd59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 10) (5000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "#X, y = make_classification(n_samples=3000, n_informative = 4, n_features=10, n_classes=5, random_state=1)\n",
    "\n",
    "X,y =make_classification(n_samples=5000, n_informative = 8, n_features=10, n_redundant = 0, n_classes=5, random_state=1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Wr9zs4cFjac"
   },
   "source": [
    "We have created a dataset with m=1000 and n=10. Each training example belongs to one of 5 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1bX3RwKFuOJ"
   },
   "source": [
    "# Split and standardization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RO8Os900Fycy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)   #Split data into 70% training, 30%test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2Xr0fLiGHh_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler    #Standardize data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfJImjfMZmzO"
   },
   "source": [
    "# Converting the labels to one hot encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjeXHOMcZrM1"
   },
   "source": [
    "To convert our targets into a one hot encoding we use the Keras to_categorical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dtx2XYc_Z39t"
   },
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYyCI_EFHJo0"
   },
   "source": [
    "# **MAKING THE NEURAL NETWORK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8r3ox244Thad"
   },
   "source": [
    "##The Sequential class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJx4JnoqHgEy"
   },
   "source": [
    "The keras.models.Sequential class is a wrapper for the neural network model that treats the network as a sequence of layers. We assign it to a variable model.  Click [here](https://keras.io/api/models/sequential/) to check out the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X836dVLpIVB4"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N93HlSc1T1Jp"
   },
   "source": [
    "##The Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Aa-YdXEJJio"
   },
   "source": [
    "First, we call the Input() object of Keras, to instantiate a Keras tensor. Note that we only input the number of features and not the training size. So the shape argument of the Input() object is inputs (n, None) \n",
    "\n",
    "To add a certain object or layer to the Sequential model, we use the *add* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGWqM_HiJrRX"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.Input(shape=10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2gEqpjJUI2X"
   },
   "source": [
    "##Adding layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELKmipHsKUTo"
   },
   "source": [
    "To add a layer to a neural network, we add a dense layer, or the Dense class. Arguments to the Dense class include the number of neurons for the layer, the activation, regularizer and and other stuff which we do not need to worry about. \n",
    "\n",
    "In this model we will make an NN with 2 hidden layers, each activated by the relu function. The last layer , which is the output layer, has 5 neurons(one belonging to each class), activated by the softmax function. \n",
    "\n",
    "[Dense layer documentation link](https://keras.io/api/layers/core_layers/dense/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJBbxgEALDsT"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=30, activation='relu' )) #First hidden layer with 30 neurons, relu activation\n",
    "model.add(tf.keras.layers.Dense(units=15, activation='relu')) #Second hidden layer with 15 neurons, relu activation\n",
    "model.add(tf.keras.layers.Dense(units=5, activation='softmax')) #Output layer with 5 neuron, softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGnQ4HLEWm3M"
   },
   "source": [
    "##Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rli-9UekVftL"
   },
   "source": [
    "Note: Here we pass the type of activation as an argument to the Dense class. This is the same as adding an Activation layer with the activation inside it. So the first line of code above is the same as:\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation(tf.keras.activations.relu))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gr8wmFs1W8FG"
   },
   "source": [
    "Click [here](https://keras.io/api/layers/activations/) to check out the various activations provided by keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDJy4tpfXIZi"
   },
   "source": [
    "##Model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNv3zOtWLjvO"
   },
   "source": [
    "Just that many lines of code to prototype our neural network model! Simple right? We can check out the model's details by calling the summary() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "4T6Pa9FbLuBU",
    "outputId": "ebfd9db4-171b-4bb7-feb1-c0f573111b51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsMp9jtxXgFB"
   },
   "source": [
    "## The compile() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLmun9IpL7Ip"
   },
   "source": [
    "Next, we compile the model using the compile() method. Here we mention what our loss function should be, the type of optimizer used, and what metrics to print.\n",
    "\n",
    "We will be using the categorical cross-entropy as our loss function, with the adam optimizer and accuracy as our metric. Note that we pass our metrics as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMZ7TEqTMPns"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzogFygcX0ED"
   },
   "source": [
    "## Keras Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaQ6H21PX3Dz"
   },
   "source": [
    "Passing optimizer='adam' to the compile method uses the default learning rate and some other parameters. But often , we want to choose the learning rate, momentum, etc. So, we call the optimizer's class, store it in a variable and pass it as an argument to the compile method.\n",
    "\n",
    "For example, if we want to use stochastic gradient descent with a learning rate of 0.1 and momentum 0.9 , we would code it as follows:\n",
    "\n",
    "\n",
    "```\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer =opt, metrics=['accuracy'])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBmwz-qBZOV_"
   },
   "source": [
    "Click [here](https://keras.io/api/optimizers/) to check out the various Keras optimizers and their syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kyDJpKAZY3a"
   },
   "source": [
    "## The fit() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2zNL_SGNL2V"
   },
   "source": [
    "Finally , we call the fit() function to fit our data! Keras takes care of all the mathematical computation of forward prop and backprop! The fit() function takes into input the data, true labels, batch size in which we train, the number of epochs, and the validation data(here which is our test set). It returns a Keras history object which contains various attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Yx8qroZnNxjc",
    "outputId": "ca52c3a0-3b7f-42e4-bbd5-2aad61bb75f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 1.4336 - accuracy: 0.4043 - val_loss: 1.2625 - val_accuracy: 0.5440\n",
      "Epoch 2/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 1.0532 - accuracy: 0.6286 - val_loss: 0.9733 - val_accuracy: 0.6360\n",
      "Epoch 3/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.8438 - accuracy: 0.6940 - val_loss: 0.8492 - val_accuracy: 0.6927\n",
      "Epoch 4/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.7364 - accuracy: 0.7397 - val_loss: 0.7651 - val_accuracy: 0.7120\n",
      "Epoch 5/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.7666 - val_loss: 0.7019 - val_accuracy: 0.7413\n",
      "Epoch 6/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.7834 - val_loss: 0.6632 - val_accuracy: 0.7540\n",
      "Epoch 7/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.5789 - accuracy: 0.7949 - val_loss: 0.6298 - val_accuracy: 0.7780\n",
      "Epoch 8/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.5506 - accuracy: 0.8089 - val_loss: 0.6064 - val_accuracy: 0.7793\n",
      "Epoch 9/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.5291 - accuracy: 0.8174 - val_loss: 0.5889 - val_accuracy: 0.7867\n",
      "Epoch 10/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.5092 - accuracy: 0.8240 - val_loss: 0.5702 - val_accuracy: 0.7920\n",
      "Epoch 11/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4901 - accuracy: 0.8329 - val_loss: 0.5569 - val_accuracy: 0.7927\n",
      "Epoch 12/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4790 - accuracy: 0.8366 - val_loss: 0.5392 - val_accuracy: 0.8060\n",
      "Epoch 13/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4660 - accuracy: 0.8386 - val_loss: 0.5281 - val_accuracy: 0.8067\n",
      "Epoch 14/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4552 - accuracy: 0.8443 - val_loss: 0.5203 - val_accuracy: 0.8127\n",
      "Epoch 15/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4438 - accuracy: 0.8520 - val_loss: 0.5176 - val_accuracy: 0.8167\n",
      "Epoch 16/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4371 - accuracy: 0.8526 - val_loss: 0.5015 - val_accuracy: 0.8213\n",
      "Epoch 17/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4262 - accuracy: 0.8551 - val_loss: 0.4930 - val_accuracy: 0.8213\n",
      "Epoch 18/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4197 - accuracy: 0.8589 - val_loss: 0.4985 - val_accuracy: 0.8240\n",
      "Epoch 19/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4119 - accuracy: 0.8620 - val_loss: 0.4911 - val_accuracy: 0.8247\n",
      "Epoch 20/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4074 - accuracy: 0.8620 - val_loss: 0.4831 - val_accuracy: 0.8260\n",
      "Epoch 21/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.4001 - accuracy: 0.8694 - val_loss: 0.4780 - val_accuracy: 0.8287\n",
      "Epoch 22/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3934 - accuracy: 0.8680 - val_loss: 0.4771 - val_accuracy: 0.8260\n",
      "Epoch 23/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3882 - accuracy: 0.8737 - val_loss: 0.4847 - val_accuracy: 0.8287\n",
      "Epoch 24/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3855 - accuracy: 0.8677 - val_loss: 0.4716 - val_accuracy: 0.8300\n",
      "Epoch 25/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3805 - accuracy: 0.8740 - val_loss: 0.4731 - val_accuracy: 0.8327\n",
      "Epoch 26/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3766 - accuracy: 0.8757 - val_loss: 0.4684 - val_accuracy: 0.8353\n",
      "Epoch 27/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3729 - accuracy: 0.8760 - val_loss: 0.4631 - val_accuracy: 0.8353\n",
      "Epoch 28/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3701 - accuracy: 0.8774 - val_loss: 0.4586 - val_accuracy: 0.8413\n",
      "Epoch 29/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3652 - accuracy: 0.8774 - val_loss: 0.4637 - val_accuracy: 0.8367\n",
      "Epoch 30/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3619 - accuracy: 0.8826 - val_loss: 0.4622 - val_accuracy: 0.8420\n",
      "Epoch 31/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3574 - accuracy: 0.8823 - val_loss: 0.4606 - val_accuracy: 0.8373\n",
      "Epoch 32/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3572 - accuracy: 0.8809 - val_loss: 0.4560 - val_accuracy: 0.8387\n",
      "Epoch 33/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.3536 - accuracy: 0.8831 - val_loss: 0.4621 - val_accuracy: 0.8387\n",
      "Epoch 34/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3517 - accuracy: 0.8806 - val_loss: 0.4593 - val_accuracy: 0.8413\n",
      "Epoch 35/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8857 - val_loss: 0.4530 - val_accuracy: 0.8420\n",
      "Epoch 36/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3458 - accuracy: 0.8877 - val_loss: 0.4573 - val_accuracy: 0.8400\n",
      "Epoch 37/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3413 - accuracy: 0.8883 - val_loss: 0.4617 - val_accuracy: 0.8393\n",
      "Epoch 38/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3396 - accuracy: 0.8880 - val_loss: 0.4609 - val_accuracy: 0.8413\n",
      "Epoch 39/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3369 - accuracy: 0.8891 - val_loss: 0.4510 - val_accuracy: 0.8393\n",
      "Epoch 40/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3340 - accuracy: 0.8889 - val_loss: 0.4564 - val_accuracy: 0.8393\n",
      "Epoch 41/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3314 - accuracy: 0.8917 - val_loss: 0.4553 - val_accuracy: 0.8393\n",
      "Epoch 42/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3301 - accuracy: 0.8926 - val_loss: 0.4526 - val_accuracy: 0.8447\n",
      "Epoch 43/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3291 - accuracy: 0.8914 - val_loss: 0.4525 - val_accuracy: 0.8420\n",
      "Epoch 44/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3261 - accuracy: 0.8943 - val_loss: 0.4499 - val_accuracy: 0.8367\n",
      "Epoch 45/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3228 - accuracy: 0.8937 - val_loss: 0.4515 - val_accuracy: 0.8453\n",
      "Epoch 46/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3235 - accuracy: 0.8920 - val_loss: 0.4481 - val_accuracy: 0.8440\n",
      "Epoch 47/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3190 - accuracy: 0.8943 - val_loss: 0.4489 - val_accuracy: 0.8460\n",
      "Epoch 48/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3160 - accuracy: 0.8991 - val_loss: 0.4510 - val_accuracy: 0.8447\n",
      "Epoch 49/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3155 - accuracy: 0.8963 - val_loss: 0.4582 - val_accuracy: 0.8447\n",
      "Epoch 50/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3151 - accuracy: 0.8929 - val_loss: 0.4576 - val_accuracy: 0.8387\n",
      "Epoch 51/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3130 - accuracy: 0.8969 - val_loss: 0.4519 - val_accuracy: 0.8427\n",
      "Epoch 52/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3122 - accuracy: 0.8977 - val_loss: 0.4571 - val_accuracy: 0.8413\n",
      "Epoch 53/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3107 - accuracy: 0.8951 - val_loss: 0.4604 - val_accuracy: 0.8453\n",
      "Epoch 54/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3086 - accuracy: 0.8991 - val_loss: 0.4576 - val_accuracy: 0.8400\n",
      "Epoch 55/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3079 - accuracy: 0.8986 - val_loss: 0.4454 - val_accuracy: 0.8373\n",
      "Epoch 56/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3047 - accuracy: 0.8997 - val_loss: 0.4551 - val_accuracy: 0.8453\n",
      "Epoch 57/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3044 - accuracy: 0.9014 - val_loss: 0.4595 - val_accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3021 - accuracy: 0.9031 - val_loss: 0.4587 - val_accuracy: 0.8467\n",
      "Epoch 59/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3015 - accuracy: 0.9014 - val_loss: 0.4570 - val_accuracy: 0.8413\n",
      "Epoch 60/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.3002 - accuracy: 0.8991 - val_loss: 0.4539 - val_accuracy: 0.8453\n",
      "Epoch 61/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2987 - accuracy: 0.9031 - val_loss: 0.4602 - val_accuracy: 0.8453\n",
      "Epoch 62/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2983 - accuracy: 0.8994 - val_loss: 0.4497 - val_accuracy: 0.8513\n",
      "Epoch 63/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2959 - accuracy: 0.9000 - val_loss: 0.4586 - val_accuracy: 0.8473\n",
      "Epoch 64/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2963 - accuracy: 0.9029 - val_loss: 0.4532 - val_accuracy: 0.8473\n",
      "Epoch 65/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2948 - accuracy: 0.9040 - val_loss: 0.4492 - val_accuracy: 0.8407\n",
      "Epoch 66/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2946 - accuracy: 0.9009 - val_loss: 0.4518 - val_accuracy: 0.8433\n",
      "Epoch 67/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2904 - accuracy: 0.9040 - val_loss: 0.4650 - val_accuracy: 0.8427\n",
      "Epoch 68/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2905 - accuracy: 0.9023 - val_loss: 0.4541 - val_accuracy: 0.8440\n",
      "Epoch 69/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2909 - accuracy: 0.9063 - val_loss: 0.4524 - val_accuracy: 0.8467\n",
      "Epoch 70/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2884 - accuracy: 0.9066 - val_loss: 0.4527 - val_accuracy: 0.8433\n",
      "Epoch 71/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.2874 - accuracy: 0.9029 - val_loss: 0.4576 - val_accuracy: 0.8513\n",
      "Epoch 72/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2870 - accuracy: 0.9071 - val_loss: 0.4594 - val_accuracy: 0.8453\n",
      "Epoch 73/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2857 - accuracy: 0.9063 - val_loss: 0.4726 - val_accuracy: 0.8380\n",
      "Epoch 74/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2841 - accuracy: 0.9071 - val_loss: 0.4551 - val_accuracy: 0.8493\n",
      "Epoch 75/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2827 - accuracy: 0.9040 - val_loss: 0.4609 - val_accuracy: 0.8413\n",
      "Epoch 76/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2817 - accuracy: 0.9026 - val_loss: 0.4631 - val_accuracy: 0.8527\n",
      "Epoch 77/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2828 - accuracy: 0.9031 - val_loss: 0.4546 - val_accuracy: 0.8487\n",
      "Epoch 78/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2812 - accuracy: 0.9014 - val_loss: 0.4563 - val_accuracy: 0.8507\n",
      "Epoch 79/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2794 - accuracy: 0.9091 - val_loss: 0.4541 - val_accuracy: 0.8380\n",
      "Epoch 80/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2783 - accuracy: 0.9077 - val_loss: 0.4532 - val_accuracy: 0.8460\n",
      "Epoch 81/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2786 - accuracy: 0.9057 - val_loss: 0.4518 - val_accuracy: 0.8500\n",
      "Epoch 82/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2785 - accuracy: 0.9063 - val_loss: 0.4510 - val_accuracy: 0.8493\n",
      "Epoch 83/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2772 - accuracy: 0.9083 - val_loss: 0.4582 - val_accuracy: 0.8493\n",
      "Epoch 84/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2739 - accuracy: 0.9114 - val_loss: 0.4654 - val_accuracy: 0.8467\n",
      "Epoch 85/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2759 - accuracy: 0.9077 - val_loss: 0.4560 - val_accuracy: 0.8453\n",
      "Epoch 86/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2745 - accuracy: 0.9077 - val_loss: 0.4499 - val_accuracy: 0.8533\n",
      "Epoch 87/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2706 - accuracy: 0.9089 - val_loss: 0.4556 - val_accuracy: 0.8480\n",
      "Epoch 88/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2715 - accuracy: 0.9120 - val_loss: 0.4580 - val_accuracy: 0.8487\n",
      "Epoch 89/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2708 - accuracy: 0.9137 - val_loss: 0.4613 - val_accuracy: 0.8500\n",
      "Epoch 90/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2722 - accuracy: 0.9054 - val_loss: 0.4597 - val_accuracy: 0.8453\n",
      "Epoch 91/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2698 - accuracy: 0.9131 - val_loss: 0.4601 - val_accuracy: 0.8460\n",
      "Epoch 92/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2675 - accuracy: 0.9123 - val_loss: 0.4580 - val_accuracy: 0.8480\n",
      "Epoch 93/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2661 - accuracy: 0.9086 - val_loss: 0.4673 - val_accuracy: 0.8387\n",
      "Epoch 94/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2672 - accuracy: 0.9111 - val_loss: 0.4659 - val_accuracy: 0.8380\n",
      "Epoch 95/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2649 - accuracy: 0.9103 - val_loss: 0.4608 - val_accuracy: 0.8487\n",
      "Epoch 96/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2639 - accuracy: 0.9094 - val_loss: 0.4566 - val_accuracy: 0.8493\n",
      "Epoch 97/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2652 - accuracy: 0.9126 - val_loss: 0.4563 - val_accuracy: 0.8480\n",
      "Epoch 98/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2633 - accuracy: 0.9131 - val_loss: 0.4566 - val_accuracy: 0.8507\n",
      "Epoch 99/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2606 - accuracy: 0.9097 - val_loss: 0.4614 - val_accuracy: 0.8447\n",
      "Epoch 100/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2627 - accuracy: 0.9094 - val_loss: 0.4699 - val_accuracy: 0.8473\n",
      "Epoch 101/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2611 - accuracy: 0.9149 - val_loss: 0.4544 - val_accuracy: 0.8473\n",
      "Epoch 102/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2608 - accuracy: 0.9149 - val_loss: 0.4670 - val_accuracy: 0.8453\n",
      "Epoch 103/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2581 - accuracy: 0.9109 - val_loss: 0.4586 - val_accuracy: 0.8453\n",
      "Epoch 104/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2577 - accuracy: 0.9114 - val_loss: 0.4595 - val_accuracy: 0.8473\n",
      "Epoch 105/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2573 - accuracy: 0.9134 - val_loss: 0.4638 - val_accuracy: 0.8487\n",
      "Epoch 106/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2567 - accuracy: 0.9123 - val_loss: 0.4568 - val_accuracy: 0.8547\n",
      "Epoch 107/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2569 - accuracy: 0.9146 - val_loss: 0.4567 - val_accuracy: 0.8513\n",
      "Epoch 108/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2558 - accuracy: 0.9131 - val_loss: 0.4679 - val_accuracy: 0.8433\n",
      "Epoch 109/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2530 - accuracy: 0.9154 - val_loss: 0.4669 - val_accuracy: 0.8487\n",
      "Epoch 110/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2550 - accuracy: 0.9177 - val_loss: 0.4631 - val_accuracy: 0.8507\n",
      "Epoch 111/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2521 - accuracy: 0.9183 - val_loss: 0.4663 - val_accuracy: 0.8433\n",
      "Epoch 112/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2537 - accuracy: 0.9160 - val_loss: 0.4648 - val_accuracy: 0.8533\n",
      "Epoch 113/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2506 - accuracy: 0.9194 - val_loss: 0.4697 - val_accuracy: 0.8513\n",
      "Epoch 114/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2510 - accuracy: 0.9163 - val_loss: 0.4623 - val_accuracy: 0.8533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2486 - accuracy: 0.9186 - val_loss: 0.4743 - val_accuracy: 0.8533\n",
      "Epoch 116/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2498 - accuracy: 0.9149 - val_loss: 0.4762 - val_accuracy: 0.8460\n",
      "Epoch 117/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2482 - accuracy: 0.9146 - val_loss: 0.4700 - val_accuracy: 0.8460\n",
      "Epoch 118/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2462 - accuracy: 0.9183 - val_loss: 0.4713 - val_accuracy: 0.8507\n",
      "Epoch 119/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2462 - accuracy: 0.9191 - val_loss: 0.4723 - val_accuracy: 0.8487\n",
      "Epoch 120/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2446 - accuracy: 0.9183 - val_loss: 0.4741 - val_accuracy: 0.8440\n",
      "Epoch 121/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2446 - accuracy: 0.9220 - val_loss: 0.4717 - val_accuracy: 0.8507\n",
      "Epoch 122/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2429 - accuracy: 0.9177 - val_loss: 0.4722 - val_accuracy: 0.8520\n",
      "Epoch 123/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2436 - accuracy: 0.9191 - val_loss: 0.4687 - val_accuracy: 0.8473\n",
      "Epoch 124/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2439 - accuracy: 0.9180 - val_loss: 0.4935 - val_accuracy: 0.8487\n",
      "Epoch 125/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2405 - accuracy: 0.9257 - val_loss: 0.4713 - val_accuracy: 0.8493\n",
      "Epoch 126/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2399 - accuracy: 0.9229 - val_loss: 0.4796 - val_accuracy: 0.8513\n",
      "Epoch 127/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2412 - accuracy: 0.9180 - val_loss: 0.4744 - val_accuracy: 0.8480\n",
      "Epoch 128/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2398 - accuracy: 0.9186 - val_loss: 0.4734 - val_accuracy: 0.8440\n",
      "Epoch 129/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2411 - accuracy: 0.9186 - val_loss: 0.4706 - val_accuracy: 0.8460\n",
      "Epoch 130/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2377 - accuracy: 0.9209 - val_loss: 0.4697 - val_accuracy: 0.8520\n",
      "Epoch 131/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2383 - accuracy: 0.9234 - val_loss: 0.4759 - val_accuracy: 0.8500\n",
      "Epoch 132/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2393 - accuracy: 0.9189 - val_loss: 0.4798 - val_accuracy: 0.8567\n",
      "Epoch 133/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2356 - accuracy: 0.9234 - val_loss: 0.4757 - val_accuracy: 0.8453\n",
      "Epoch 134/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2385 - accuracy: 0.9226 - val_loss: 0.4792 - val_accuracy: 0.8540\n",
      "Epoch 135/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2358 - accuracy: 0.9240 - val_loss: 0.4725 - val_accuracy: 0.8513\n",
      "Epoch 136/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2357 - accuracy: 0.9197 - val_loss: 0.4690 - val_accuracy: 0.8493\n",
      "Epoch 137/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2339 - accuracy: 0.9206 - val_loss: 0.4917 - val_accuracy: 0.8493\n",
      "Epoch 138/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2319 - accuracy: 0.9234 - val_loss: 0.4774 - val_accuracy: 0.8500\n",
      "Epoch 139/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2344 - accuracy: 0.9229 - val_loss: 0.4774 - val_accuracy: 0.8540\n",
      "Epoch 140/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2311 - accuracy: 0.9229 - val_loss: 0.4792 - val_accuracy: 0.8487\n",
      "Epoch 141/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2320 - accuracy: 0.9203 - val_loss: 0.4834 - val_accuracy: 0.8467\n",
      "Epoch 142/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2331 - accuracy: 0.9257 - val_loss: 0.4774 - val_accuracy: 0.8520\n",
      "Epoch 143/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2303 - accuracy: 0.9249 - val_loss: 0.4841 - val_accuracy: 0.8413\n",
      "Epoch 144/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2298 - accuracy: 0.9234 - val_loss: 0.4826 - val_accuracy: 0.8513\n",
      "Epoch 145/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2295 - accuracy: 0.9240 - val_loss: 0.4897 - val_accuracy: 0.8540\n",
      "Epoch 146/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2277 - accuracy: 0.9243 - val_loss: 0.4928 - val_accuracy: 0.8527\n",
      "Epoch 147/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2269 - accuracy: 0.9254 - val_loss: 0.4820 - val_accuracy: 0.8467\n",
      "Epoch 148/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2298 - accuracy: 0.9223 - val_loss: 0.4924 - val_accuracy: 0.8453\n",
      "Epoch 149/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2272 - accuracy: 0.9260 - val_loss: 0.4849 - val_accuracy: 0.8480\n",
      "Epoch 150/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2264 - accuracy: 0.9229 - val_loss: 0.4996 - val_accuracy: 0.8380\n",
      "Epoch 151/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2253 - accuracy: 0.9229 - val_loss: 0.4922 - val_accuracy: 0.8507\n",
      "Epoch 152/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2279 - accuracy: 0.9240 - val_loss: 0.4997 - val_accuracy: 0.8453\n",
      "Epoch 153/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2258 - accuracy: 0.9277 - val_loss: 0.4929 - val_accuracy: 0.8487\n",
      "Epoch 154/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2241 - accuracy: 0.9223 - val_loss: 0.4956 - val_accuracy: 0.8433\n",
      "Epoch 155/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2241 - accuracy: 0.9243 - val_loss: 0.4910 - val_accuracy: 0.8513\n",
      "Epoch 156/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2210 - accuracy: 0.9266 - val_loss: 0.4953 - val_accuracy: 0.8460\n",
      "Epoch 157/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2231 - accuracy: 0.9269 - val_loss: 0.4968 - val_accuracy: 0.8487\n",
      "Epoch 158/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2216 - accuracy: 0.9234 - val_loss: 0.4912 - val_accuracy: 0.8520\n",
      "Epoch 159/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2232 - accuracy: 0.9266 - val_loss: 0.4905 - val_accuracy: 0.8560\n",
      "Epoch 160/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2210 - accuracy: 0.9286 - val_loss: 0.4930 - val_accuracy: 0.8527\n",
      "Epoch 161/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2196 - accuracy: 0.9266 - val_loss: 0.5145 - val_accuracy: 0.8453\n",
      "Epoch 162/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2173 - accuracy: 0.9271 - val_loss: 0.5018 - val_accuracy: 0.8460\n",
      "Epoch 163/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2203 - accuracy: 0.9257 - val_loss: 0.5001 - val_accuracy: 0.8553\n",
      "Epoch 164/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2191 - accuracy: 0.9263 - val_loss: 0.4928 - val_accuracy: 0.8480\n",
      "Epoch 165/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2171 - accuracy: 0.9283 - val_loss: 0.5084 - val_accuracy: 0.8433\n",
      "Epoch 166/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2182 - accuracy: 0.9271 - val_loss: 0.4906 - val_accuracy: 0.8487\n",
      "Epoch 167/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2157 - accuracy: 0.9291 - val_loss: 0.4899 - val_accuracy: 0.8520\n",
      "Epoch 168/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2156 - accuracy: 0.9277 - val_loss: 0.4923 - val_accuracy: 0.8553\n",
      "Epoch 169/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2154 - accuracy: 0.9271 - val_loss: 0.4923 - val_accuracy: 0.8560\n",
      "Epoch 170/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2135 - accuracy: 0.9317 - val_loss: 0.4925 - val_accuracy: 0.8473\n",
      "Epoch 171/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2130 - accuracy: 0.9286 - val_loss: 0.4916 - val_accuracy: 0.8513\n",
      "Epoch 172/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2140 - accuracy: 0.9294 - val_loss: 0.5021 - val_accuracy: 0.8427\n",
      "Epoch 173/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2123 - accuracy: 0.9280 - val_loss: 0.5078 - val_accuracy: 0.8487\n",
      "Epoch 174/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2137 - accuracy: 0.9254 - val_loss: 0.4944 - val_accuracy: 0.8547\n",
      "Epoch 175/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2112 - accuracy: 0.9297 - val_loss: 0.4915 - val_accuracy: 0.8540\n",
      "Epoch 176/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2115 - accuracy: 0.9297 - val_loss: 0.4972 - val_accuracy: 0.8493\n",
      "Epoch 177/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2096 - accuracy: 0.9297 - val_loss: 0.4985 - val_accuracy: 0.8547\n",
      "Epoch 178/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2097 - accuracy: 0.9277 - val_loss: 0.4959 - val_accuracy: 0.8533\n",
      "Epoch 179/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2082 - accuracy: 0.9314 - val_loss: 0.5021 - val_accuracy: 0.8540\n",
      "Epoch 180/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2103 - accuracy: 0.9277 - val_loss: 0.4963 - val_accuracy: 0.8520\n",
      "Epoch 181/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2090 - accuracy: 0.9303 - val_loss: 0.5001 - val_accuracy: 0.8513\n",
      "Epoch 182/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2067 - accuracy: 0.9317 - val_loss: 0.5007 - val_accuracy: 0.8553\n",
      "Epoch 183/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2072 - accuracy: 0.9303 - val_loss: 0.5047 - val_accuracy: 0.8553\n",
      "Epoch 184/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2053 - accuracy: 0.9291 - val_loss: 0.5024 - val_accuracy: 0.8547\n",
      "Epoch 185/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2048 - accuracy: 0.9294 - val_loss: 0.5016 - val_accuracy: 0.8540\n",
      "Epoch 186/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2066 - accuracy: 0.9331 - val_loss: 0.5090 - val_accuracy: 0.8487\n",
      "Epoch 187/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2060 - accuracy: 0.9286 - val_loss: 0.5069 - val_accuracy: 0.8493\n",
      "Epoch 188/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2080 - accuracy: 0.9291 - val_loss: 0.5049 - val_accuracy: 0.8480\n",
      "Epoch 189/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2063 - accuracy: 0.9309 - val_loss: 0.5074 - val_accuracy: 0.8473\n",
      "Epoch 190/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2035 - accuracy: 0.9326 - val_loss: 0.5063 - val_accuracy: 0.8547\n",
      "Epoch 191/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2052 - accuracy: 0.9274 - val_loss: 0.4996 - val_accuracy: 0.8513\n",
      "Epoch 192/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2024 - accuracy: 0.9323 - val_loss: 0.5174 - val_accuracy: 0.8433\n",
      "Epoch 193/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2028 - accuracy: 0.9343 - val_loss: 0.5075 - val_accuracy: 0.8473\n",
      "Epoch 194/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2025 - accuracy: 0.9300 - val_loss: 0.5103 - val_accuracy: 0.8487\n",
      "Epoch 195/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.2017 - accuracy: 0.9326 - val_loss: 0.5148 - val_accuracy: 0.8527\n",
      "Epoch 196/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2007 - accuracy: 0.9303 - val_loss: 0.5092 - val_accuracy: 0.8493\n",
      "Epoch 197/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2017 - accuracy: 0.9306 - val_loss: 0.5061 - val_accuracy: 0.8493\n",
      "Epoch 198/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2015 - accuracy: 0.9311 - val_loss: 0.4986 - val_accuracy: 0.8513\n",
      "Epoch 199/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.2003 - accuracy: 0.9294 - val_loss: 0.5107 - val_accuracy: 0.8540\n",
      "Epoch 200/200\n",
      "219/219 [==============================] - 1s 3ms/step - loss: 0.1977 - accuracy: 0.9323 - val_loss: 0.5066 - val_accuracy: 0.8540\n"
     ]
    }
   ],
   "source": [
    "keras_history = model.fit(X_train, y_train , batch_size=16, epochs=200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUkigFLQcAfL"
   },
   "source": [
    "##The Keras History Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJRNfnw6cGaT"
   },
   "source": [
    "The Keras history object has two main attributes- \n",
    "\n",
    "1)model \n",
    "\n",
    "2)history\n",
    "\n",
    "The model contains all information about the weights, inputs, activations and so on\n",
    "\n",
    "For example, to get the weights you would code:\n",
    "\n",
    "\n",
    "```\n",
    "keras_history.model.get_weights()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1AnMOSFhg20"
   },
   "source": [
    "The history is a dictionary containing training loss, training accuracy, validation loss, validation accuracy, etc. after each epoch. Each key contains an array of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vBUz_lDQh2y1",
    "outputId": "1231a5e6-58ca-4527-fa2f-000a6e648c74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4vq5LXY1iyfU"
   },
   "source": [
    "## Post training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/vats/anaconda3/envs/deeplearning/lib/python3.8/site-packages (3.2.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/vats/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vats/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/vats/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/vats/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/vats/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: six in /home/vats/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwQOaGb2jCPX"
   },
   "source": [
    "Now we plot the training and test loss with each epoch. A key observation to make is that our model is overfitting- training accuracy is 94% while test accuracy is 87%. Notice how the training loss decreases while the test loss increases after a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "qMSG1xPvOAoI",
    "outputId": "9745afe7-ac6f-4cfa-e0af-4c3ab57f18eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff44c608bb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(keras_history.history['loss'], label='Training loss')\n",
    "plt.plot(keras_history.history['val_loss'], color='red', label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkvqbRxUjewo"
   },
   "source": [
    "#**IMPROVING YOUR NEURAL NETWORK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h96i8MOKjlPb"
   },
   "source": [
    "Great! Now you know how to build a basic artificial neural network in Keras. However, ANN's are extremely prone to overfitting, so always keep a close tab on that. We will now see how various methods are deployed in keras to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KF4Hu4CVj9AK"
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0M6IXEmkBUt"
   },
   "source": [
    "You can apply an L1, L2 or both L1 and L2 regularizers to a layer using tf.keras.regularizers. The value of $\\lambda$ is specified as an argument of the class. For example\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dense(units=30, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1egJo7ylNrQ"
   },
   "source": [
    "Click [here](https://keras.io/api/layers/regularizers/#l1-class) to check out the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UXYeM_PlYvj"
   },
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwZfBan3lkgw"
   },
   "source": [
    "Nothing complicated here. Just do this:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Jdr1DsTlw86"
   },
   "source": [
    "[Link for BatchNorm](https://keras.io/api/layers/normalization_layers/batch_normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa6aX97wmB2f"
   },
   "source": [
    "##Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jA6A5QBRmEsi"
   },
   "source": [
    "Nothing complicated here too. The probability of a neuron being \"dropped out\" is specified. Higher the value, the \"simpler\" the neural network is. But make sure not to specify too high a dropout rate else the model will underfit\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBalrMLBnXep"
   },
   "source": [
    "#**PUTTING IT ALL TOGETHER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXTTGh_5nZ_M"
   },
   "source": [
    "Here is all of the concepts we have just learnt. Please note that all the abovementioned techniques to prevent overfitting have been implemented below, to get an idea of how they are used in Keras. In reality, all these are not necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "dQDQMwlBjd9N",
    "outputId": "ad70f7c3-9e8d-4080-d127-b5ec83f1fed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15)                60        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 1,055\n",
      "Trainable params: 965\n",
      "Non-trainable params: 90\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=10,))\n",
    "model.add(tf.keras.layers.Dense(units=30, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01))) \n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.125))\n",
    "model.add(tf.keras.layers.Dense(units=15, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01))) \n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(units=5, activation='softmax')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6b86CyX5qWB4",
    "outputId": "69944b9e-5f5f-4210-c3b0-cc229b6472d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 1.8484 - accuracy: 0.3583 - val_loss: 1.4414 - val_accuracy: 0.4980\n",
      "Epoch 2/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 1.4794 - accuracy: 0.4840 - val_loss: 1.2884 - val_accuracy: 0.5587\n",
      "Epoch 3/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 1.3659 - accuracy: 0.5089 - val_loss: 1.1858 - val_accuracy: 0.6027\n",
      "Epoch 4/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 1.2654 - accuracy: 0.5457 - val_loss: 1.1000 - val_accuracy: 0.6433\n",
      "Epoch 5/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 1.2206 - accuracy: 0.5817 - val_loss: 1.0330 - val_accuracy: 0.6867\n",
      "Epoch 6/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 1.1638 - accuracy: 0.5917 - val_loss: 0.9646 - val_accuracy: 0.7053\n",
      "Epoch 7/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 1.0905 - accuracy: 0.6149 - val_loss: 0.9075 - val_accuracy: 0.7247\n",
      "Epoch 8/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 1.0440 - accuracy: 0.6454 - val_loss: 0.8703 - val_accuracy: 0.7367\n",
      "Epoch 9/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 1.0152 - accuracy: 0.6486 - val_loss: 0.8305 - val_accuracy: 0.7413\n",
      "Epoch 10/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.9833 - accuracy: 0.6589 - val_loss: 0.7980 - val_accuracy: 0.7480\n",
      "Epoch 11/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.9563 - accuracy: 0.6583 - val_loss: 0.7622 - val_accuracy: 0.7567\n",
      "Epoch 12/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.9441 - accuracy: 0.6797 - val_loss: 0.7416 - val_accuracy: 0.7613\n",
      "Epoch 13/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.9108 - accuracy: 0.6903 - val_loss: 0.7223 - val_accuracy: 0.7733\n",
      "Epoch 14/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.9011 - accuracy: 0.6829 - val_loss: 0.7079 - val_accuracy: 0.7727\n",
      "Epoch 15/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.8760 - accuracy: 0.7006 - val_loss: 0.6960 - val_accuracy: 0.7733\n",
      "Epoch 16/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.8669 - accuracy: 0.7063 - val_loss: 0.6712 - val_accuracy: 0.7827\n",
      "Epoch 17/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.8709 - accuracy: 0.6997 - val_loss: 0.6700 - val_accuracy: 0.7880\n",
      "Epoch 18/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.8443 - accuracy: 0.7114 - val_loss: 0.6567 - val_accuracy: 0.7887\n",
      "Epoch 19/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.8381 - accuracy: 0.7129 - val_loss: 0.6491 - val_accuracy: 0.7893\n",
      "Epoch 20/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.8571 - accuracy: 0.7043 - val_loss: 0.6429 - val_accuracy: 0.7900\n",
      "Epoch 21/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.8293 - accuracy: 0.7171 - val_loss: 0.6358 - val_accuracy: 0.7847\n",
      "Epoch 22/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.8058 - accuracy: 0.7263 - val_loss: 0.6302 - val_accuracy: 0.7947\n",
      "Epoch 23/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.8147 - accuracy: 0.7186 - val_loss: 0.6277 - val_accuracy: 0.7953\n",
      "Epoch 24/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7992 - accuracy: 0.7226 - val_loss: 0.6145 - val_accuracy: 0.7980\n",
      "Epoch 25/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7785 - accuracy: 0.7243 - val_loss: 0.6014 - val_accuracy: 0.7947\n",
      "Epoch 26/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.8008 - accuracy: 0.7286 - val_loss: 0.5935 - val_accuracy: 0.8000\n",
      "Epoch 27/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7781 - accuracy: 0.7286 - val_loss: 0.5950 - val_accuracy: 0.8020\n",
      "Epoch 28/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7771 - accuracy: 0.7303 - val_loss: 0.5801 - val_accuracy: 0.7987\n",
      "Epoch 29/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7642 - accuracy: 0.7437 - val_loss: 0.5741 - val_accuracy: 0.7980\n",
      "Epoch 30/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7664 - accuracy: 0.7366 - val_loss: 0.5698 - val_accuracy: 0.8093\n",
      "Epoch 31/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7539 - accuracy: 0.7343 - val_loss: 0.5611 - val_accuracy: 0.8147\n",
      "Epoch 32/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7552 - accuracy: 0.7323 - val_loss: 0.5495 - val_accuracy: 0.8220\n",
      "Epoch 33/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7376 - accuracy: 0.7494 - val_loss: 0.5591 - val_accuracy: 0.8133\n",
      "Epoch 34/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7422 - accuracy: 0.7389 - val_loss: 0.5553 - val_accuracy: 0.8187\n",
      "Epoch 35/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7504 - accuracy: 0.7426 - val_loss: 0.5507 - val_accuracy: 0.8180\n",
      "Epoch 36/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7514 - accuracy: 0.7383 - val_loss: 0.5508 - val_accuracy: 0.8240\n",
      "Epoch 37/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7223 - accuracy: 0.7457 - val_loss: 0.5393 - val_accuracy: 0.8193\n",
      "Epoch 38/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7406 - accuracy: 0.7443 - val_loss: 0.5483 - val_accuracy: 0.8233\n",
      "Epoch 39/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7289 - accuracy: 0.7471 - val_loss: 0.5438 - val_accuracy: 0.8160\n",
      "Epoch 40/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7159 - accuracy: 0.7471 - val_loss: 0.5477 - val_accuracy: 0.8227\n",
      "Epoch 41/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7348 - accuracy: 0.7523 - val_loss: 0.5414 - val_accuracy: 0.8193\n",
      "Epoch 42/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7223 - accuracy: 0.7506 - val_loss: 0.5413 - val_accuracy: 0.8200\n",
      "Epoch 43/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.7098 - accuracy: 0.7529 - val_loss: 0.5289 - val_accuracy: 0.8267\n",
      "Epoch 44/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7167 - accuracy: 0.7454 - val_loss: 0.5488 - val_accuracy: 0.8207\n",
      "Epoch 45/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7066 - accuracy: 0.7503 - val_loss: 0.5210 - val_accuracy: 0.8413\n",
      "Epoch 46/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7086 - accuracy: 0.7549 - val_loss: 0.5240 - val_accuracy: 0.8260\n",
      "Epoch 47/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7007 - accuracy: 0.7500 - val_loss: 0.5257 - val_accuracy: 0.8227\n",
      "Epoch 48/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7098 - accuracy: 0.7497 - val_loss: 0.5325 - val_accuracy: 0.8187\n",
      "Epoch 49/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6939 - accuracy: 0.7537 - val_loss: 0.5213 - val_accuracy: 0.8280\n",
      "Epoch 50/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7096 - accuracy: 0.7597 - val_loss: 0.5145 - val_accuracy: 0.8307\n",
      "Epoch 51/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7036 - accuracy: 0.7537 - val_loss: 0.5148 - val_accuracy: 0.8287\n",
      "Epoch 52/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7211 - accuracy: 0.7449 - val_loss: 0.5190 - val_accuracy: 0.8260\n",
      "Epoch 53/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6971 - accuracy: 0.7566 - val_loss: 0.5138 - val_accuracy: 0.8253\n",
      "Epoch 54/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7025 - accuracy: 0.7546 - val_loss: 0.5145 - val_accuracy: 0.8247\n",
      "Epoch 55/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7134 - accuracy: 0.7420 - val_loss: 0.5148 - val_accuracy: 0.8307\n",
      "Epoch 56/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6894 - accuracy: 0.7503 - val_loss: 0.5176 - val_accuracy: 0.8247\n",
      "Epoch 57/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7029 - accuracy: 0.7489 - val_loss: 0.5179 - val_accuracy: 0.8320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6980 - accuracy: 0.7549 - val_loss: 0.5060 - val_accuracy: 0.8260\n",
      "Epoch 59/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6909 - accuracy: 0.7557 - val_loss: 0.5114 - val_accuracy: 0.8267\n",
      "Epoch 60/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6995 - accuracy: 0.7540 - val_loss: 0.5034 - val_accuracy: 0.8307\n",
      "Epoch 61/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6750 - accuracy: 0.7654 - val_loss: 0.5114 - val_accuracy: 0.8240\n",
      "Epoch 62/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.7617 - val_loss: 0.5057 - val_accuracy: 0.8313\n",
      "Epoch 63/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6741 - accuracy: 0.7717 - val_loss: 0.5028 - val_accuracy: 0.8273\n",
      "Epoch 64/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7020 - accuracy: 0.7623 - val_loss: 0.5066 - val_accuracy: 0.8320\n",
      "Epoch 65/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6820 - accuracy: 0.7637 - val_loss: 0.5037 - val_accuracy: 0.8387\n",
      "Epoch 66/200\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.6821 - accuracy: 0.7563 - val_loss: 0.5081 - val_accuracy: 0.8287\n",
      "Epoch 67/200\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.6808 - accuracy: 0.7626 - val_loss: 0.4983 - val_accuracy: 0.8307\n",
      "Epoch 68/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6945 - accuracy: 0.7606 - val_loss: 0.4939 - val_accuracy: 0.8333\n",
      "Epoch 69/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6961 - accuracy: 0.7511 - val_loss: 0.5002 - val_accuracy: 0.8313\n",
      "Epoch 70/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6872 - accuracy: 0.7494 - val_loss: 0.5017 - val_accuracy: 0.8313\n",
      "Epoch 71/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6793 - accuracy: 0.7557 - val_loss: 0.4887 - val_accuracy: 0.8413\n",
      "Epoch 72/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6767 - accuracy: 0.7674 - val_loss: 0.4851 - val_accuracy: 0.8393\n",
      "Epoch 73/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.7697 - val_loss: 0.4995 - val_accuracy: 0.8373\n",
      "Epoch 74/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6897 - accuracy: 0.7543 - val_loss: 0.4973 - val_accuracy: 0.8307\n",
      "Epoch 75/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6731 - accuracy: 0.7580 - val_loss: 0.4884 - val_accuracy: 0.8313\n",
      "Epoch 76/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6676 - accuracy: 0.7654 - val_loss: 0.4818 - val_accuracy: 0.8373\n",
      "Epoch 77/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.7711 - val_loss: 0.4905 - val_accuracy: 0.8327\n",
      "Epoch 78/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6699 - accuracy: 0.7606 - val_loss: 0.4794 - val_accuracy: 0.8440\n",
      "Epoch 79/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6618 - accuracy: 0.7614 - val_loss: 0.4833 - val_accuracy: 0.8347\n",
      "Epoch 80/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6685 - accuracy: 0.7611 - val_loss: 0.4772 - val_accuracy: 0.8387\n",
      "Epoch 81/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6674 - accuracy: 0.7629 - val_loss: 0.4835 - val_accuracy: 0.8367\n",
      "Epoch 82/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6699 - accuracy: 0.7654 - val_loss: 0.4820 - val_accuracy: 0.8380\n",
      "Epoch 83/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6667 - accuracy: 0.7646 - val_loss: 0.4729 - val_accuracy: 0.8467\n",
      "Epoch 84/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.7683 - val_loss: 0.4802 - val_accuracy: 0.8333\n",
      "Epoch 85/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6715 - accuracy: 0.7683 - val_loss: 0.4802 - val_accuracy: 0.8347\n",
      "Epoch 86/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6376 - accuracy: 0.7714 - val_loss: 0.5048 - val_accuracy: 0.8307\n",
      "Epoch 87/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.7660 - val_loss: 0.4903 - val_accuracy: 0.8333\n",
      "Epoch 88/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6742 - accuracy: 0.7617 - val_loss: 0.4861 - val_accuracy: 0.8287\n",
      "Epoch 89/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6801 - accuracy: 0.7549 - val_loss: 0.4776 - val_accuracy: 0.8387\n",
      "Epoch 90/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6739 - accuracy: 0.7534 - val_loss: 0.4772 - val_accuracy: 0.8360\n",
      "Epoch 91/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.7674 - val_loss: 0.4678 - val_accuracy: 0.8347\n",
      "Epoch 92/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6722 - accuracy: 0.7663 - val_loss: 0.4755 - val_accuracy: 0.8373\n",
      "Epoch 93/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.7691 - val_loss: 0.4751 - val_accuracy: 0.8320\n",
      "Epoch 94/200\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6814 - accuracy: 0.7583 - val_loss: 0.4791 - val_accuracy: 0.8313\n",
      "Epoch 95/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6741 - accuracy: 0.7626 - val_loss: 0.4759 - val_accuracy: 0.8393\n",
      "Epoch 96/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6683 - accuracy: 0.7663 - val_loss: 0.4737 - val_accuracy: 0.8367\n",
      "Epoch 97/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.7726 - val_loss: 0.4685 - val_accuracy: 0.8347\n",
      "Epoch 98/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6580 - accuracy: 0.7694 - val_loss: 0.4649 - val_accuracy: 0.8420\n",
      "Epoch 99/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.7714 - val_loss: 0.4752 - val_accuracy: 0.8313\n",
      "Epoch 100/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.7689 - val_loss: 0.4683 - val_accuracy: 0.8427\n",
      "Epoch 101/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6618 - accuracy: 0.7626 - val_loss: 0.4783 - val_accuracy: 0.8287\n",
      "Epoch 102/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.7720 - val_loss: 0.4730 - val_accuracy: 0.8320\n",
      "Epoch 103/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.7711 - val_loss: 0.4668 - val_accuracy: 0.8433\n",
      "Epoch 104/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6388 - accuracy: 0.7723 - val_loss: 0.4667 - val_accuracy: 0.8440\n",
      "Epoch 105/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6707 - accuracy: 0.7700 - val_loss: 0.4612 - val_accuracy: 0.8347\n",
      "Epoch 106/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.7677 - val_loss: 0.4639 - val_accuracy: 0.8453\n",
      "Epoch 107/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.7614 - val_loss: 0.4733 - val_accuracy: 0.8333\n",
      "Epoch 108/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6430 - accuracy: 0.7740 - val_loss: 0.4733 - val_accuracy: 0.8347\n",
      "Epoch 109/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.7714 - val_loss: 0.4761 - val_accuracy: 0.8353\n",
      "Epoch 110/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.7734 - val_loss: 0.4601 - val_accuracy: 0.8433\n",
      "Epoch 111/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6510 - accuracy: 0.7711 - val_loss: 0.4656 - val_accuracy: 0.8313\n",
      "Epoch 112/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.7677 - val_loss: 0.4785 - val_accuracy: 0.8307\n",
      "Epoch 113/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.7737 - val_loss: 0.4623 - val_accuracy: 0.8420\n",
      "Epoch 114/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6572 - accuracy: 0.7611 - val_loss: 0.4714 - val_accuracy: 0.8407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.7734 - val_loss: 0.4630 - val_accuracy: 0.8460\n",
      "Epoch 116/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.7703 - val_loss: 0.4671 - val_accuracy: 0.8347\n",
      "Epoch 117/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.7780 - val_loss: 0.4631 - val_accuracy: 0.8387\n",
      "Epoch 118/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.7737 - val_loss: 0.4542 - val_accuracy: 0.8527\n",
      "Epoch 119/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.7734 - val_loss: 0.4661 - val_accuracy: 0.8373\n",
      "Epoch 120/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6510 - accuracy: 0.7697 - val_loss: 0.4747 - val_accuracy: 0.8407\n",
      "Epoch 121/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.7709 - val_loss: 0.4610 - val_accuracy: 0.8427\n",
      "Epoch 122/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6112 - accuracy: 0.7877 - val_loss: 0.4671 - val_accuracy: 0.8360\n",
      "Epoch 123/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.7637 - val_loss: 0.4543 - val_accuracy: 0.8413\n",
      "Epoch 124/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.7640 - val_loss: 0.4636 - val_accuracy: 0.8460\n",
      "Epoch 125/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6544 - accuracy: 0.7594 - val_loss: 0.4534 - val_accuracy: 0.8500\n",
      "Epoch 126/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.7714 - val_loss: 0.4566 - val_accuracy: 0.8507\n",
      "Epoch 127/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6626 - accuracy: 0.7634 - val_loss: 0.4576 - val_accuracy: 0.8440\n",
      "Epoch 128/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6456 - accuracy: 0.7709 - val_loss: 0.4515 - val_accuracy: 0.8400\n",
      "Epoch 129/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6682 - accuracy: 0.7711 - val_loss: 0.4702 - val_accuracy: 0.8347\n",
      "Epoch 130/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.7834 - val_loss: 0.4633 - val_accuracy: 0.8433\n",
      "Epoch 131/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.7597 - val_loss: 0.4676 - val_accuracy: 0.8400\n",
      "Epoch 132/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6439 - accuracy: 0.7737 - val_loss: 0.4610 - val_accuracy: 0.8447\n",
      "Epoch 133/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.7720 - val_loss: 0.4549 - val_accuracy: 0.8380\n",
      "Epoch 134/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.7749 - val_loss: 0.4506 - val_accuracy: 0.8447\n",
      "Epoch 135/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6487 - accuracy: 0.7631 - val_loss: 0.4565 - val_accuracy: 0.8380\n",
      "Epoch 136/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.7769 - val_loss: 0.4549 - val_accuracy: 0.8460\n",
      "Epoch 137/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.7714 - val_loss: 0.4569 - val_accuracy: 0.8393\n",
      "Epoch 138/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.7754 - val_loss: 0.4617 - val_accuracy: 0.8433\n",
      "Epoch 139/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.7880 - val_loss: 0.4574 - val_accuracy: 0.8373\n",
      "Epoch 140/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.7686 - val_loss: 0.4578 - val_accuracy: 0.8467\n",
      "Epoch 141/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6171 - accuracy: 0.7814 - val_loss: 0.4555 - val_accuracy: 0.8480\n",
      "Epoch 142/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.7769 - val_loss: 0.4605 - val_accuracy: 0.8427\n",
      "Epoch 143/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6453 - accuracy: 0.7763 - val_loss: 0.4516 - val_accuracy: 0.8453\n",
      "Epoch 144/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.7811 - val_loss: 0.4519 - val_accuracy: 0.8493\n",
      "Epoch 145/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6283 - accuracy: 0.7800 - val_loss: 0.4513 - val_accuracy: 0.8513\n",
      "Epoch 146/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6640 - accuracy: 0.7714 - val_loss: 0.4607 - val_accuracy: 0.8360\n",
      "Epoch 147/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6677 - accuracy: 0.7660 - val_loss: 0.4572 - val_accuracy: 0.8447\n",
      "Epoch 148/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.7651 - val_loss: 0.4576 - val_accuracy: 0.8373\n",
      "Epoch 149/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.7751 - val_loss: 0.4670 - val_accuracy: 0.8480\n",
      "Epoch 150/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.7729 - val_loss: 0.4572 - val_accuracy: 0.8333\n",
      "Epoch 151/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6441 - accuracy: 0.7740 - val_loss: 0.4836 - val_accuracy: 0.8327\n",
      "Epoch 152/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.7577 - val_loss: 0.4594 - val_accuracy: 0.8387\n",
      "Epoch 153/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.7669 - val_loss: 0.4540 - val_accuracy: 0.8527\n",
      "Epoch 154/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.7700 - val_loss: 0.4550 - val_accuracy: 0.8467\n",
      "Epoch 155/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6179 - accuracy: 0.7869 - val_loss: 0.4511 - val_accuracy: 0.8373\n",
      "Epoch 156/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6093 - accuracy: 0.7843 - val_loss: 0.4711 - val_accuracy: 0.8340\n",
      "Epoch 157/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.7754 - val_loss: 0.4654 - val_accuracy: 0.8453\n",
      "Epoch 158/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.7749 - val_loss: 0.4627 - val_accuracy: 0.8347\n",
      "Epoch 159/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.7789 - val_loss: 0.4627 - val_accuracy: 0.8353\n",
      "Epoch 160/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6138 - accuracy: 0.7880 - val_loss: 0.4630 - val_accuracy: 0.8313\n",
      "Epoch 161/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6469 - accuracy: 0.7714 - val_loss: 0.4683 - val_accuracy: 0.8400\n",
      "Epoch 162/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.7689 - val_loss: 0.4527 - val_accuracy: 0.8380\n",
      "Epoch 163/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6486 - accuracy: 0.7706 - val_loss: 0.4530 - val_accuracy: 0.8440\n",
      "Epoch 164/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.7691 - val_loss: 0.4538 - val_accuracy: 0.8373\n",
      "Epoch 165/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.7746 - val_loss: 0.4572 - val_accuracy: 0.8433\n",
      "Epoch 166/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.7717 - val_loss: 0.4604 - val_accuracy: 0.8433\n",
      "Epoch 167/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.7749 - val_loss: 0.4591 - val_accuracy: 0.8400\n",
      "Epoch 168/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.7691 - val_loss: 0.4572 - val_accuracy: 0.8360\n",
      "Epoch 169/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.7814 - val_loss: 0.4514 - val_accuracy: 0.8433\n",
      "Epoch 170/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.7686 - val_loss: 0.4604 - val_accuracy: 0.8447\n",
      "Epoch 171/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.7829 - val_loss: 0.4505 - val_accuracy: 0.8407\n",
      "Epoch 172/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6110 - accuracy: 0.7823 - val_loss: 0.4376 - val_accuracy: 0.8533\n",
      "Epoch 173/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.7743 - val_loss: 0.4480 - val_accuracy: 0.8420\n",
      "Epoch 174/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.7866 - val_loss: 0.4532 - val_accuracy: 0.8353\n",
      "Epoch 175/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.7777 - val_loss: 0.4460 - val_accuracy: 0.8413\n",
      "Epoch 176/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.7817 - val_loss: 0.4528 - val_accuracy: 0.8487\n",
      "Epoch 177/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6764 - accuracy: 0.7600 - val_loss: 0.4538 - val_accuracy: 0.8460\n",
      "Epoch 178/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.7686 - val_loss: 0.4569 - val_accuracy: 0.8413\n",
      "Epoch 179/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6375 - accuracy: 0.7660 - val_loss: 0.4486 - val_accuracy: 0.8467\n",
      "Epoch 180/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6511 - accuracy: 0.7723 - val_loss: 0.4557 - val_accuracy: 0.8380\n",
      "Epoch 181/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.7763 - val_loss: 0.4501 - val_accuracy: 0.8407\n",
      "Epoch 182/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6473 - accuracy: 0.7663 - val_loss: 0.4591 - val_accuracy: 0.8493\n",
      "Epoch 183/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6249 - accuracy: 0.7766 - val_loss: 0.4580 - val_accuracy: 0.8453\n",
      "Epoch 184/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6442 - accuracy: 0.7637 - val_loss: 0.4601 - val_accuracy: 0.8433\n",
      "Epoch 185/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.7909 - val_loss: 0.4482 - val_accuracy: 0.8447\n",
      "Epoch 186/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.7829 - val_loss: 0.4508 - val_accuracy: 0.8427\n",
      "Epoch 187/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.7789 - val_loss: 0.4431 - val_accuracy: 0.8427\n",
      "Epoch 188/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6324 - accuracy: 0.7811 - val_loss: 0.4443 - val_accuracy: 0.8527\n",
      "Epoch 189/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.7691 - val_loss: 0.4547 - val_accuracy: 0.8413\n",
      "Epoch 190/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.7809 - val_loss: 0.4463 - val_accuracy: 0.8400\n",
      "Epoch 191/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6127 - accuracy: 0.7871 - val_loss: 0.4524 - val_accuracy: 0.8400\n",
      "Epoch 192/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.7800 - val_loss: 0.4427 - val_accuracy: 0.8467\n",
      "Epoch 193/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6125 - accuracy: 0.7763 - val_loss: 0.4502 - val_accuracy: 0.8460\n",
      "Epoch 194/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.7737 - val_loss: 0.4496 - val_accuracy: 0.8400\n",
      "Epoch 195/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.7649 - val_loss: 0.4571 - val_accuracy: 0.8467\n",
      "Epoch 196/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.7746 - val_loss: 0.4415 - val_accuracy: 0.8527\n",
      "Epoch 197/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6297 - accuracy: 0.7763 - val_loss: 0.4485 - val_accuracy: 0.8387\n",
      "Epoch 198/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.7726 - val_loss: 0.4635 - val_accuracy: 0.8347\n",
      "Epoch 199/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.7757 - val_loss: 0.4450 - val_accuracy: 0.8447\n",
      "Epoch 200/200\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.7751 - val_loss: 0.4653 - val_accuracy: 0.8373\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "keras_history = model.fit(X_train, y_train , batch_size=16, epochs=200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "1BpghfAMqcEW",
    "outputId": "fc570e9a-d05f-447c-a698-3c80d651104d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff44c35b250>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(keras_history.history['loss'], label='Training loss')\n",
    "plt.plot(keras_history.history['val_loss'], color='red', label='Test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1B1b4YcoMoY"
   },
   "source": [
    "**We now urge you to tweak various hyperparamers, such as the learning rate,  dropout rate, regularization parameter, batch size, number of layers, number of neurons, etc. to get an idea of how the model performs with such changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNvrOIEypT9v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "biJIIaHWpUCw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHgbYB8QpUk5"
   },
   "source": [
    "Awesome! Hope you've gotten the gist of how to use Keras and how simple and straightforward it is to use."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KerasIntro",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
